<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<title data-react-helmet="true">From ML training to Geant4 inference | ML4Sim</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://your-docusaurus-test-site.com/docs/G4_Inference/from_training_to_inference"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="From ML training to Geant4 inference | ML4Sim"><meta data-react-helmet="true" name="description" content="Once the model is trained, tested and validated, it is then deployed in a production framework. ML deployment involves placing this ML model into an environment where it can perform inference for fast shower simulation. This environment is Geant4."><meta data-react-helmet="true" property="og:description" content="Once the model is trained, tested and validated, it is then deployed in a production framework. ML deployment involves placing this ML model into an environment where it can perform inference for fast shower simulation. This environment is Geant4."><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://your-docusaurus-test-site.com/docs/G4_Inference/from_training_to_inference"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/G4_Inference/from_training_to_inference" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/G4_Inference/from_training_to_inference" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.7069df34.css">
<link rel="preload" href="/assets/js/runtime~main.71701159.js" as="script">
<link rel="preload" href="/assets/js/main.f9c432a9.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.png" alt="Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">ML4Sim</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Get started</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">üåú</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">üåû</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Full and fast simulation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ml_fastsim">Generative modeling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ml_workflow">Machine learning workflow</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/ML_Model/training">Generative models for fast simulation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/G4_Inference/from_training_to_inference">Inference integration in Geant4</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/G4_Inference/from_training_to_inference">From ML training to Geant4 inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/G4_Inference/G4_examples">Geant4 examples</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/G4_Inference/inference_optimization">Inference optimization</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>From ML training to Geant4 inference</h1><p>Once the model is trained, tested and validated, it is then deployed in a production framework. ML deployment involves placing this ML model into an environment where it can perform inference for fast shower simulation. This environment is Geant4. </p><p>The ML model trained in a python envirnment needs to be first converted to to a format ready to use in C++. The process of model deployment in Geant4 uses external libraries used for ML inference such as LWTNN and ONNXRuntime. Fast simulation components such as an inference model are implemented. </p><h2 class="anchor anchorWithStickyNavbar_mojV" id="inference-libraries">Inference libraries<a class="hash-link" href="#inference-libraries" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="lwtnn">LWTNN<a class="hash-link" href="#lwtnn" title="Direct link to heading">‚Äã</a></h3><p>Lightweight Trained Neural Network or <strong><a href="https://github.com/lwtnn/lwtnn" target="_blank" rel="noopener noreferrer">LWTNN</a></strong> is a C++ library to apply neural networks. It has minimal dependencies: Eigen and Boost. It loads and applies the saved model (as a JSON file) using LWTNN in C++. </p><p>After mdoel training, to save it as a format that can be used for inference in C++, it should be saved as two separate files of the architecture (in JSON) and the weights (in HDF5). This operation can be done (in Python) with:</p><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># save the architecture in a JSON file</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">with open</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;architecture.json&#x27;</span><span class="token plain">, </span><span class="token string" style="color:#e3116c">&#x27;w&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> as arch_file:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    arch_file.write</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model.to_json</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">))</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># save the weights as an HDF5 file</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model.save_weights</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;weights.h5&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>After building the LWTNN code available at this <strong><a href="https://github.com/lwtnn/lwtnn" target="_blank" rel="noopener noreferrer">link</a></strong>, run the <strong>kerasfunc2json</strong> python script (available in lwtnn/converters/) to generate a template file of your functional model input variables by calling:</p><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">$ kerasfunc2json.py architecture.json weights.h5 </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> inputs.json</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>And run again <strong>kerasfunc2json</strong> script to get your output file that would be used for the inference in C++:</p><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">$ kerasfunc2json.py architecture.json weights.h5 inputs.json </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> Generator.json</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>The object that will do the computation in this class is a <strong>LightweightGraph</strong>, initialized from Generator.json file. The inference is based on evaluating the graph using an input inference vector.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="onnxruntime">ONNXRuntime<a class="hash-link" href="#onnxruntime" title="Direct link to heading">‚Äã</a></h3><p>Open Neural Network Exchange Runtime or <strong><a href="https://github.com/microsoft/onnxruntime" target="_blank" rel="noopener noreferrer">ONNXRuntime</a></strong> is a framework for neural networks inference.</p><p>After training, to save a model as a format that can be used for inference in C++, for a Keras model for example, first it should be saved as HDF5 file with:</p><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">model.save</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;model.h5&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This model is then converted into an ONNX format using the model converted <strong><a href="https://github.com/onnx/keras-onnx" target="_blank" rel="noopener noreferrer">keras2onnx</a></strong> with: </p><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Create the Keras model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kerasModel </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tensorflow.keras.models.load_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">‚Äúmodel.h5‚Äù</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Convert Keras model into an ONNX model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">onnxModel </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> keras2onnx.convert_keras</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"> kerasModel , ‚Äòname‚Äô </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Save the ONNX model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">keras2onnx.save_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">onnxModel, ‚ÄòGenerator.onnx&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>The inference code should create an <strong>environment</strong> which manages an internal thread pool and creates as well the <strong>inference session</strong> for the model. This session runs the inference using an input vector.</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Geant4 inference in C++</h5></div><div class="admonition-content"><p>Check in the next page the <strong><a href="/docs/G4_Inference/G4_examples">Par04 example</a></strong> to see the implementation of the inference in C++ using LWTNN and ONNXRuntime</p></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="comparison-between-lwtnn-and-onnxruntime">Comparison between LWTNN and ONNXRuntime<a class="hash-link" href="#comparison-between-lwtnn-and-onnxruntime" title="Direct link to heading">‚Äã</a></h3><p>The table below summarizes the difference between in the two inference libraries in terms of supported ML libraries, disk space and memory footrpint when using the same model to perform inference in Geant4. </p><table><thead><tr><th></th><th>LWTNN</th><th>ONNXRuntime</th></tr></thead><tbody><tr><td>Supported ML libraries</td><td>Saves only Keras and Sklearn models</td><td>Saves models from (almost) all libraries</td></tr><tr><td>Supported layers</td><td>All expect:  CNN, Repeat vector, Reshape.</td><td>All</td></tr><tr><td>Supported Activation functions</td><td>All except: Selu, PRelu</td><td>All</td></tr><tr><td>File format</td><td>JSON</td><td>ONNX</td></tr><tr><td>Disk space (MB)</td><td>195</td><td>28.3</td></tr><tr><td>Resident memory (MB)</td><td>4000</td><td>61</td></tr><tr><td>Virtual memory (MB)</td><td>4000</td><td>52</td></tr></tbody></table><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tensorflow models and LWTNN</h5></div><div class="admonition-content"><p>It is also possible to convert a tensorflow model into a Keras model and then use it with LWTNN</p></div></div><p>For the training, the detector considered is a homogeneous cylinder of lead (PBWO4). It is segmented along (r,<!-- -->œÜ<!-- -->,z)=(24,24,24) to create a readout geometry in the cylindrical coordinates. The model is conditonned on the energy of the incident particle, where a flat energy range is considered for the training going from 1 GeV to 100 GeV. After training the same model is converted into a JSON file and into ONNX. For reference, the disk space for the weights, saved as hdf5 file, is 28.3 MB and the model&#x27;s architecture, saved as a JSON file is 5.71 kB. The memory footprint of the model presented in the table represents the evaluation of the <strong>inference in C++</strong>. For <strong>LWTNN</strong>, it represents the memory used to compute the graph. For <strong>ONNXRuntime</strong>, it represents the memory used to run the inference session.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git/docs/G4_Inference/from_training_to_inference.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/ML_Model/optimization"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Model optimization</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/G4_Inference/G4_examples"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Geant4 examples</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#inference-libraries" class="table-of-contents__link toc-highlight">Inference libraries</a><ul><li><a href="#lwtnn" class="table-of-contents__link toc-highlight">LWTNN</a></li><li><a href="#onnxruntime" class="table-of-contents__link toc-highlight">ONNXRuntime</a></li><li><a href="#comparison-between-lwtnn-and-onnxruntime" class="table-of-contents__link toc-highlight">Comparison between LWTNN and ONNXRuntime</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Get started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://indico.cern.ch/category/13860/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>ML4Sim<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://mattermost.web.cern.ch/ml4sim/channels/town-square" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Mattermost<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2022, CERN</div></div></div></footer></div>
<script src="/assets/js/runtime~main.71701159.js"></script>
<script src="/assets/js/main.f9c432a9.js"></script>
</body>
</html>